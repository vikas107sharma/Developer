RATE LIMITING: TOKEN BUCKET 
THROTTLING: LEACKY BUCKET



1️⃣ Leaky Bucket Implementation

Explain how the Leaky Bucket algorithm works and write pseudocode (or Python code) to throttle requests at a fixed rate of 10 requests/second.

2️⃣ Token Bucket with Burst Capacity

Design a Token Bucket rate limiter that allows a user to burst up to 50 requests instantly but sustain only 5 requests/second afterward. How would you implement this in Redis?

3️⃣ Per-User Quotas (100 calls/minute)

Implement a rate limiter that allows each user up to 100 API calls per minute. Show how you’d store counters (and expiry) efficiently to enforce this across multiple app servers.

4️⃣ Sliding Window vs Fixed Window

Compare Fixed Window vs Sliding Window counters. Given a traffic spike at the boundary, which one prevents unfair throttling? Implement a Sliding Window in code.

5️⃣ Distributed Rate Limiting

Your API runs on 20 instances behind a load balancer. Design a distributed rate limiting system (using Redis, Kafka, or another shared store) that ensures consistent limits across all instances.





1️⃣ Leaky Bucket — throttle to 10 req/sec

Idea: treat requests as water drops into a bucket that leaks at a constant rate. If bucket overflows, reject.

Properties: smooths bursts, enforces steady outflow.

Simple in-memory Python (single-process):

import time
import threading

class LeakyBucket:
    def __init__(self, rate_per_sec=10, capacity=10):
        self.rate = rate_per_sec
        self.capacity = capacity
        self.water = 0.0
        self.last = time.time()
        self.lock = threading.Lock()

    def allow(self, tokens=1):
        with self.lock:
            now = time.time()
            # leak amount since last check
            leaked = (now - self.last) * self.rate
            self.water = max(0.0, self.water - leaked)
            self.last = now

            if self.water + tokens <= self.capacity:
                self.water += tokens
                return True
            return False

# usage
bucket = LeakyBucket(rate_per_sec=10, capacity=10)
if bucket.allow():
    # process request
    pass
else:
    # reject (429)
    pass


Notes: in-memory only — not suitable for multi-instance. Choose capacity to allow short bursts.

2️⃣ Token Bucket with Burst (burst=50, sustain=5/sec) — Redis implementation

Idea: bucket gets tokens at rate r (5/sec) up to capacity (50). Request consumes tokens. Allows bursts up to capacity.

Why Redis? centralized across processes; use Lua for atomicity.

Redis Lua script + Python usage:

-- token_bucket.lua
-- KEYS[1] = key
-- ARGV[1] = now (ms)
-- ARGV[2] = rate_per_sec (tokens/sec)
-- ARGV[3] = capacity
-- ARGV[4] = tokens_requested

local key = KEYS[1]
local now = tonumber(ARGV[1])
local rate = tonumber(ARGV[2])
local capacity = tonumber(ARGV[3])
local req = tonumber(ARGV[4])

local data = redis.call("HMGET", key, "tokens", "ts")
local tokens = tonumber(data[1]) or capacity
local ts = tonumber(data[2]) or now

-- add tokens
local elapsed = (now - ts) / 1000.0
tokens = math.min(capacity, tokens + elapsed * rate)
if tokens >= req then
  tokens = tokens - req
  redis.call("HMSET", key, "tokens", tokens, "ts", now)
  redis.call("PEXPIRE", key, 60000) -- expire after idle 60s
  return 1
else
  redis.call("HMSET", key, "tokens", tokens, "ts", now)
  redis.call("PEXPIRE", key, 60000)
  return 0
end


Python (uses redis-py)

import redis, time

r = redis.Redis()

# load script once
with open("token_bucket.lua", "r") as f:
    lua = f.read()
script = r.register_script(lua)

def allow(user_key, rate=5, capacity=50, req=1):
    now_ms = int(time.time() * 1000)
    return script(keys=[f"tb:{user_key}"],
                  args=[now_ms, rate, capacity, req]) == 1

# use
if allow("user:123"):
    # serve
    pass
else:
    # 429
    pass


Notes: atomic, supports burst and sustain rates. Tune expiry and storage.

3️⃣ Per-user quota: 100 calls/minute (Fixed Window) — Redis efficient counters

Approach: INCR a key quota:{user}:{YYYYMMDDHHMM} with EXPIRE = 61s. Fast & simple.

Pros: cheap, easy.
Cons: boundary effects (double bursting at minute edges).

Implementation:

import time
import redis

r = redis.Redis()

def allow_fixed(user_id, limit=100):
    key = f"quota:{user_id}:{int(time.time() // 60)}"
    count = r.incr(key)
    if count == 1:
        r.expire(key, 61)   # auto-expire after 61s
    return count <= limit

# usage
if allow_fixed("u123"):
    # ok
    pass
else:
    # 429
    pass


Scale: works across many app instances because Redis is shared.

4️⃣ Sliding Window vs Fixed Window — implement Sliding Window (accurate)

Concept: store timestamps of requests in a Redis sorted set (ZADD timestamp -> member). On request, remove old timestamps, count entries; allow if count < limit.

Pros: fair near boundaries, more accurate.
Cons: more storage and CPU per request.

Implementation (Redis Lua for atomicity):

-- sliding_window.lua
-- KEYS[1] = key (e.g., sw:{user})
-- ARGV[1] = now_ms
-- ARGV[2] = window_ms
-- ARGV[3] = limit

local key = KEYS[1]
local now = tonumber(ARGV[1])
local window = tonumber(ARGV[2])
local limit = tonumber(ARGV[3])
local min = now - window

-- remove old entries
redis.call('ZREMRANGEBYSCORE', key, 0, min)

-- current count
local cnt = redis.call('ZCARD', key)
if cnt < limit then
  -- add current event with score now (use unique member)
  redis.call('ZADD', key, now, now .. "-" .. tostring(math.random()))
  redis.call('PEXPIRE', key, window + 1000)
  return 1
else
  return 0
end


Python loader:

import redis, time
r = redis.Redis()
lua = open("sliding_window.lua").read()
script = r.register_script(lua)

def allow_sliding(user, limit=100, window_sec=60):
    now_ms = int(time.time() * 1000)
    return script(keys=[f"sw:{user}"], args=[now_ms, window_sec*1000, limit]) == 1


Which prevents unfair throttling? Sliding window prevents boundary bursts that fixed window allows.

5️⃣ Distributed Rate Limiting across 20 instances

Design options (preferred):

Centralized Redis-based limiter (Token Bucket or Sliding Window) with atomic operations (Lua) — low latency, easy to scale.

Or use a dedicated rate-limit service (Envoy, Nginx with limit_req, or a dedicated Redis cluster).

For very high throughput, use local leaky/token buckets + periodic global reconciliation (but more complex).

Recommended solution: Redis token-bucket via Lua (like #2) — single source of truth, atomic, low latency. Use sharding/clustered Redis for scale.

Practical considerations:

Keep scripts atomic (Lua) to avoid race conditions.

Set proper TTL to auto-expire idle keys.

Add metrics + circuit-breakers: if Redis is down, decide fallback policy: allow-all, deny-all, or use local fail-safe (e.g., reduced local rate).

Use client-side caching of tokens for microsecond decisions, refresh from Redis periodically to reduce Redis QPS (optimisation).

Failover / resilience pattern (example):

Primary enforcement via Redis Lua.

On Redis error: fallback to local token bucket with conservative limits to avoid overload.

Monitor Redis latency and errors; alert.

Example hybrid pseudo-flow:

on request:
  try:
    call Redis token-bucket Lua -> atomic allow/deny
  except RedisError:
    use local in-process token bucket (smaller capacity) as emergency fallback

Quick tradeoff summary

Leaky bucket: smooths traffic, simple, single-process only.

Token bucket: supports bursts, easy to implement in Redis+Lua for distributed use.

Fixed window (INCR+EXPIRE): simplest, cheap, boundary issues.

Sliding window (ZSET): accurate and fair, heavier on storage/CPU.

Distributed: use Redis (atomic Lua scripts) or edge proxies (Envoy/Nginx). Plan fallback if Redis fails.